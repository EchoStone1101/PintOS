#include "mm.h"
#include "frame.h"
#include "mapfile.h"
#include "swap.h"
#include "threads/palloc.h"
#include "threads/malloc.h"
#include "threads/synch.h"
#include "threads/vaddr.h"
#include "threads/thread.h"
#include "userprog/pagedir.h"
#include "devices/timer.h"
#include <list.h>
#include <hash.h>
#include <string.h>
#include <stdio.h>

extern struct lock filesys_lock;
extern struct frame *frame_table;
extern struct lock frame_table_lock;

extern size_t frame_table_size;
extern void *userpool_base;


/** Convert a frame to its actual physical address, mapped as kernel
    virtual memory. */
void *
frame_to_phys (struct frame *f)
{
	ASSERT (f >= frame_table && f < frame_table + frame_table_size);
	off_t offset = (off_t)f - (off_t)frame_table;
	ASSERT (offset % sizeof (struct frame) == 0);

	return userpool_base + (offset / sizeof (struct frame)) * PGSIZE;
}

/** Convert a physical address in userpool to corresponding frame. */
struct frame *
phys_to_frame (void * phys_addr)
{
	ASSERT (phys_addr >= userpool_base &&
					phys_addr < userpool_base + frame_table_size * PGSIZE);
	ASSERT ((int)phys_addr % PGSIZE == 0);

	return &frame_table[(phys_addr - userpool_base) / PGSIZE];
}

/** Allocate an empty frame from frame_table. The frame content 
    is NOT ZEROED, where the caller should properly set it.
    Does not grab frame_table_lock, because FRAME struct is untouched.
 		Returns NULL if no frame is available. */
struct frame *
frame_alloc (void)
{
	void *phys_addr = palloc_get_page (PAL_USER);
	if (phys_addr != NULL)
		{
			struct frame *f = phys_to_frame (phys_addr);
			ASSERT (frame_is_empty (f) && frame_pinned (f) == 0
																 && !frame_is_busy (f));
			return f;
		}
	return NULL;
}

/** Free the given frame, in terms of both FRAME struct and underlying
    physical frame, using auxiliary data AUX. The operation done on frame 
		content is specified by the free_operations member of FRAME, which 
		the caller should set up properly.

    frame_table_lock is expected to be held before calling, and it is 
		atomatically RELEASED at return. */
void
frame_free (struct frame *f, int aux)
{
	ASSERT (lock_held_by_current_thread (&frame_table_lock));

	/* PINNED or BUSY frames are certainly referenced and not freed.
	   Freeing an empty frame is also an error. */
	ASSERT (!frame_is_busy (f));
	ASSERT (!frame_is_empty (f));
	ASSERT (!frame_pinned (f));
	
#ifdef FRAME_DEBUG
	printf ("%d freed frame %p\n", thread_tid (), frame_to_phys (f));
#endif

	if (!frame_is_write (f))
		{
			/* Discard the content. Nothing more to do. */
			goto direct;
		}
	else
		{
			/* Write to file or swap. */
			if (frame_is_filebacked (f))
				{
					struct map_file *mapfile = frame_mapfile (f);
					ASSERT (mapfile != NULL && mapfile->writable);
				
					frame_set_busy (f);
					lock_release (&frame_table_lock);
					/* Release frame_table_lock before I/O, for more concurrency. */
					lock_acquire (&filesys_lock);

					/* Write READ_BYTES at OFFSET to backing file. */
					size_t bytes_written = file_write_at (mapfile->backing_file, frame_to_phys (f), 
											 					 aux, frame_offset (f));
					ASSERT (aux == (int)bytes_written);

					lock_release (&filesys_lock);
				}
			else
				{
					/* Write to swap space. */
					frame_set_busy (f);
					lock_release (&frame_table_lock);
					/* Release frame_table_lock before I/O, for more concurrency. */
					
					swap_put_page (aux, frame_to_phys (f));
					swap_clear_busy (aux);
					/* Now access to this page can page-in from swap again. */
				}
		}
	
	/* Clean up the frame struct. */
	lock_acquire (&frame_table_lock);
direct:
	f->mapfile = 0;
	f->offset = 0;
	/* which also clears all flags. */

	/* Free the user page. */
	void *phys_addr = frame_to_phys (f);
	palloc_free_page (phys_addr);
	lock_release (&frame_table_lock);
}

/** Check FRAME's accessed bit, based on RM_LIST generated by
    frame_reverse_map(). If CLEAR is set, also clears the first access
		bit encountered. If HARD is set, do not early stop. */
bool
frame_accessed (struct frame *f, struct list *rm_list, bool clear, bool hard)
{
	bool accessed = false;

	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);

      /* Locate PTE. */
			void *upage = pg_round_down (vma->lower_bound + 
																	(frame_offset (f) - vma->offset));
			
			if (pagedir_is_accessed (vma->proc->pagedir, upage))
				{
					accessed = true;
					if (clear)
						pagedir_set_accessed (vma->proc->pagedir, upage, false);
					if (!hard)
						break;
				}
    }
	return accessed;
}

/** Check FRAME's dirty bit, based on RM_LIST generated by
    frame_reverse_map(). If CLEAR is set, also clears all dirty
		bits in PTEs. */
bool
frame_dirty (struct frame *f, struct list *rm_list, bool clear)
{
	bool dirty = false;

	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			void *upage = pg_round_down (vma->lower_bound + 
																	(frame_offset (f) - vma->offset));
			if (pagedir_is_dirty (vma->proc->pagedir, upage))
				{
					dirty = true;
					if (clear)
						pagedir_set_dirty (vma->proc->pagedir, upage, false);
				}
    }
	return dirty;
}

/** Get the READ_BYTES property of FRAME, if file-backed, by 
    checking RM_LIST. 
		A frame should be shared by VMAs that, given FRAME's offset 
		in backing file, agree on how many bytes from that offset 
		belong to this FRAME's content. */
int
frame_get_read_bytes (struct frame *f, struct list *rm_list)
{
	int read_bytes = -1;

	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			int remaining = (int)(vma->filesize - frame_offset (f));
			ASSERT (remaining >= 0);
			int _read_bytes = remaining > PGSIZE ? PGSIZE : remaining;

			if (read_bytes == -1) 
				read_bytes = _read_bytes;
			else
				{
					ASSERT (read_bytes == _read_bytes);
				}
    }
	ASSERT (read_bytes != 0);
	return read_bytes;
}

/** Invalidate all PTEs referencing FRAME, based on RM_LIST generated by
    frame_reverse_map(), filling the PA field with AUX. */
void
frame_invalidate (struct frame *f, struct list *rm_list, int aux, int avl_flag)
{
	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			void *upage = pg_round_down (vma->lower_bound + 
																	(frame_offset (f) - vma->offset));
			
			pagedir_clear_page (vma->proc->pagedir, upage, aux, avl_flag);
    }
}


/** Build the reverse mapping from FRAME to all relevant VMAs (containing
    present PTEs referencing FRAME), recorded in RM_LIST.
		If FRAME is file-backed, the corresponding MAP_FILE's page_pool_lock
		is also held, and NOT RELEASED ON RETURN. This is meant to protect
		the list of VMAs in RM_LIST, on which the caller can operate safely.
		When done with RM_LIST, frame_reverse_map_free() must be invoked
		to possibly release the page_pool_lock. 
		
		Note that RM_LIST can be empty: when a private page is loaded and
		COW, no PTE actually reference the original frame, but the frame
		remains resident. Since reverse mapping is designed for frame_evict(),
		it should then decide to immediately free the frame, with no need of
		writing back. 
		
		Finally, if FRAME is found BUSY, busy waits until it's not. This happens
		when FRAME is being added to a new PTE, at a soft PF, which should not
		take long. */
void
frame_reverse_map (struct frame *f, struct list *rm_list)
{
	ASSERT (!frame_is_empty (f));

	if (frame_is_filebacked (f))
		{
			/* FRAME -> MAP_FILE -> VMA */
			struct map_file *mapfile = frame_mapfile (f);
			ASSERT (mapfile != NULL);

			/* Grab page_pool_lock to also protect vma_list. */
			lock_acquire (&mapfile->page_pool_lock);
			
			struct list_elem *e;
      for (e = list_begin (&mapfile->vma_list); e != list_end (&mapfile->vma_list);)
        {
          struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
          /* Does FRAME fall in the area described by VMA? */
					off_t offset = frame_offset (f);
					if (offset >= vma->offset && 
							offset < vma->offset + (vma->upper_bound - vma->lower_bound))
						{
							ASSERT (vma->proc->pagedir != NULL);
							void *phys_addr = frame_to_phys (f);
							/* We only want present PTEs that map this frame. */
							if (pagedir_get_page (vma->proc->pagedir, vma->lower_bound + offset - vma->offset)
									!= phys_addr)
								goto next;

							/* Detach from vma_list, onto RM_LIST. */
							e = list_remove (e);
							list_push_back (rm_list, &vma->mapfile_elem);
							continue;
						}
					next: 
					e = list_next (e);
        }
			/* page_pool_lock is not released! */
		}
	else if (frame_is_anon (f))
		{
			/* FRAME -> VMA */
			struct vm_area *vma = frame_anon_vma (f);
			off_t offset = frame_offset (f);
			if (offset >= vma->offset && 
					offset < vma->offset + (vma->upper_bound - vma->lower_bound))
				{
					ASSERT (vma->proc->pagedir != NULL);
					if (pagedir_get_page (vma->proc->pagedir, vma->lower_bound + offset - vma->offset) 
							!= frame_to_phys (f))
						return;
					
					if (vma_is_filebacked (vma))
						{
							lock_acquire (&vma->mapfile->page_pool_lock);
							list_remove (&vma->mapfile_elem);
							list_push_back (rm_list, &vma->mapfile_elem);
							/* page_pool_lock is not released! */
						}
					else
						list_push_back (rm_list, &vma->mapfile_elem);
				}
		}
}


/** Free up the reverse map RM_LIST built on FRAME, that is, if FRAME
    is file-backed, re-insert RM_LIST into MAP_FILE's vma_list, and 
		release page_pool_lock. */
void
frame_reverse_map_free (struct frame *f, struct list *rm_list)
{
	ASSERT (!frame_is_empty (f));
	if (frame_is_filebacked (f))
		{
			struct map_file *mapfile = frame_mapfile (f);
			ASSERT (mapfile != NULL);
			ASSERT (lock_held_by_current_thread (&mapfile->page_pool_lock));

			/* Gracefully re-insert the entire list. */
			list_splice (list_end (&mapfile->vma_list),
									 list_begin (rm_list), list_end (rm_list));
			lock_release (&mapfile->page_pool_lock);
		}
	if (frame_is_anon (f))
		{
			ASSERT (list_size (rm_list) <= 1);
			struct list_elem *e = list_empty (rm_list) ? NULL : list_begin (rm_list);
			if (e == NULL)
				return;
			struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			if (vma_is_filebacked (vma))
				{
					struct map_file *mapfile = vma->mapfile;
					ASSERT (lock_held_by_current_thread (&mapfile->page_pool_lock));
					list_remove (&vma->mapfile_elem);
					list_push_back (&mapfile->vma_list, &vma->mapfile_elem);
					lock_release (&mapfile->page_pool_lock);
				}
			else
				list_remove (&vma->mapfile_elem);
		}
}

/** Evict pages from the frame_table, so that at least CNT frames
    are free. Returns the number of pages actually evicted.
    
		The eviction policy is "clock algorithm" that approximates LFU (not LRU,
		in that pages with multiple accessed PTEs tend to stay in memory, i.e., 
		only one accessed bit is cleared at a time), with some heuristics.

		 - evict_clock_hand is maintained as the start of next eviction probing.
		 - For each frame encountered, clears the first accessed bit in its 
		   reverse mapping. If no accessed bit is found, evict.
		 - PINNED or BUSY frames are skipped.
		 - Free frames, if encountered, simply decreases the CNT requirement by 1.
		 - Early stops after scanning the entire frame_table once.

		 - With HARD, accessed bits are cleared for entire reverse mapping,
		   and the policy approximates LRU.

		On the system level, eviction happens (1) when free frame is needed but
		not found (2) every 500 ticks (5 seconds), done by evict_daemon thread 
		started by timer interrupt, which checks if frame table has more than 
		FRAME_HIGH_LEVEL entries, and evict for FRAME_LOW_LEVEL left. The case
		(1) starts without HARD, and is repeated with HARD if no single frame 
		is evicted. The case (2) is always without HARD. */
int
frame_evict (size_t cnt, bool hard)
{
	ASSERT (!lock_held_by_current_thread (&frame_table_lock));

	/* Start of next eviction probing. Protected by frame_table_lock. */
	static size_t evict_clock_hand = 0;

	size_t evicted = 0, scanned = 0;
	struct list rm_list;
	list_init (&rm_list);
	while (evicted < cnt && scanned < frame_table_size)
		{
			/* Fine-grained locking. */
			if (!lock_held_by_current_thread (&frame_table_lock))
				lock_acquire (&frame_table_lock);
			scanned++;

			struct frame *f = &frame_table[evict_clock_hand];

			/* Accept already empty frame. */
			if (frame_is_empty (f))
				{
					evicted++;
					goto next;
				}

			/* Skip pinned frame. 
				 Note that altough paging in also sets frame BUSY, it keeps empty 
				 until it acquires frame_table_lock. Either we come before that,
				 and decide this frame is empty; or we come after, then find it
				 busy. */
			if (frame_pinned (f) || frame_is_busy (f))
				goto next;

			/* Acquire the reverse map. */
			frame_reverse_map (f, &rm_list);
			if (list_empty (&rm_list))
				{
					/* Orphaned frame that has no present PTE reference. */
					ASSERT (frame_is_filebacked (f));
					ASSERT (!frame_is_write (f));
				}
			/* This check is necessary, because mapfile_get_page() might pin
			   an existing frame, after the previous PINNED check, but before
				 frame_reverse_map() holds the page_pool_lock. */
			if (frame_pinned (f))
				{
					frame_reverse_map_free (f, &rm_list);
					goto next;
				}

			bool accessed = frame_accessed (f, &rm_list, true, hard);
#ifdef FRAME_DEBUG
			printf ("frame_evict: considering frame %p, %s\n", 
							frame_to_phys (f), accessed ? "accessed" : "not accessed");
#endif
			if (accessed)
				{
					frame_reverse_map_free (f, &rm_list);
					goto next;
				}
			
			/* Properly set up PTEs. */
			int aux = 0;
			if (frame_is_filebacked (f))
				{
					/* Call frame_dirty() to clear DIRTY bits anyway. */
					if (frame_dirty (f, &rm_list, true))
						frame_set_write (f);
					
					int read_bytes = frame_get_read_bytes (f, &rm_list);
					frame_invalidate (f, &rm_list, read_bytes, AVL_INFILE);

					struct map_file *mapfile = frame_mapfile (f);
					hash_delete (&mapfile->page_pool, &f->elem);
					aux = read_bytes;
				}
			else if (frame_is_anon (f))
				{
					int slot_idx = swap_reserve_slot ();
					if (slot_idx == SWAP_SLOT_INVALID)
						{
							/* We should of course have done more to avoid finding memory
							   and swap both full, for example, by introducing OOM killing. */
							PANIC ("frame_evict: swap is full");

							frame_reverse_map_free (f, &rm_list);
							goto next;
						}
					swap_set_busy (slot_idx);
					frame_invalidate (f, &rm_list, slot_idx, AVL_INSWAP);
					/* By now, access to this frame should PF, and busy waits fetching
					   from swap, until we finish swapping out. */
					aux = slot_idx;
				}
			/* Otherwise, just discard the content. */

			/* Found eviction target. */
			evicted++;
			evict_clock_hand = (evict_clock_hand + 1) % frame_table_size;
#ifdef FRAME_DEBUG
			printf ("%d Evicting frame %p, to slot %d\n", thread_tid (), frame_to_phys (f), aux);
#endif

			frame_reverse_map_free (f, &rm_list);
			/* And now, one of accesses to FRAME decides that FRAME is not 
				 in memory, allocates a new frame, then tries to read from file or
				 swap.
				 NOTE: if frame_free() decides to write back to file, we currently 
				 don't have a way to ensure this read happens after that write. 
				 This can cause MMAPed area to be inconsistent when shared, but is
				 tolerated in Lab3. We believe this can be addressed when we tackle
				 the file system.
				 For reading/writing the swap space, however, a busy wait mechanism
				 (like BUSY bit in a frame) is built in swap.c to solve the problem. */

			/* Finally, free the frame (it atomatically releases frame_table_lock). */
			frame_free (f, aux);
			continue;

			next:
			evict_clock_hand = (evict_clock_hand + 1) % frame_table_size;
			if (scanned % EVICT_GRANULARITY == 0)
				lock_release (&frame_table_lock);
		}
	if (lock_held_by_current_thread (&frame_table_lock))
		lock_release (&frame_table_lock);
	return evicted;
}

/** Hash funcs for FRAMEs in MAP_FILE's page_pool. */
unsigned 
frame_hash_func (const struct hash_elem *e, void *aux UNUSED)
{
	struct frame *f = hash_entry (e, struct frame, elem);
	return hash_int (((int) frame_offset (f)));
}

bool 
frame_less_func (const struct hash_elem *a,
                 const struct hash_elem *b,
                 void *aux UNUSED)
{
	struct frame *fa = hash_entry (a, struct frame, elem);
	struct frame *fb = hash_entry (b, struct frame, elem);

	return frame_offset (fa) < frame_offset (fb);
}
