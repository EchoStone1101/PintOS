#include "mm.h"
#include "frame.h"
#include "mapfile.h"
#include "swap.h"
#include "threads/palloc.h"
#include "threads/malloc.h"
#include "threads/synch.h"
#include "threads/vaddr.h"
#include "threads/thread.h"
#include "userprog/pagedir.h"
#include "devices/timer.h"
#include <list.h>
#include <hash.h>
#include <string.h>
#include <stdio.h>

extern struct lock filesys_lock;
extern struct frame *frame_table;
extern struct lock frame_table_lock;

extern size_t frame_table_size;
extern void *userpool_base;


/** Convert a frame to its actual physical address, mapped as kernel
    virtual memory. */
void *
frame_to_phys (struct frame *f)
{
	ASSERT (f >= frame_table && f < frame_table + frame_table_size);
	off_t offset = (off_t)f - (off_t)frame_table;
	ASSERT (offset % sizeof (struct frame) == 0);

	return userpool_base + (offset / sizeof (struct frame)) * PGSIZE;
}

/** Convert a physical address in userpool to corresponding frame. */
struct frame *
phys_to_frame (void * phys_addr)
{
	ASSERT (phys_addr >= userpool_base &&
					phys_addr < userpool_base + frame_table_size * PGSIZE);
	ASSERT ((int)phys_addr % PGSIZE == 0);

	return &frame_table[(phys_addr - userpool_base) / PGSIZE];
}

/** Allocate an empty frame from frame_table. The frame content 
    is NOT ZEROED, where the caller should properly set it.
    Does not grab frame_table_lock, because FRAME struct is untouched.
 		Returns NULL if no frame is available. */
struct frame *
frame_alloc (void)
{
	void *phys_addr = palloc_get_page (PAL_USER);
	if (phys_addr != NULL)
		{
			struct frame *f = phys_to_frame (phys_addr);
			ASSERT (frame_is_empty (f) && frame_pinned (f) == 0
																 && !frame_is_busy (f));
			return f;
		}
	return NULL;
}

/** Free the given frame, in terms of both FRAME struct and underlying
    physical frame, using auxiliary data AUX. The operation done on frame 
		content is specified by the WRITE bit of FRAME, which the caller should 
		set up properly.

    frame_table_lock is expected to be held before calling, and it is 
		atomatically RELEASED at return. */
void
frame_free (struct frame *f, int aux)
{
	ASSERT (lock_held_by_current_thread (&frame_table_lock));

	/* Sanity checks. */
	ASSERT (!frame_is_busy (f));
	ASSERT (!frame_is_empty (f));
	ASSERT (!frame_pinned (f));
	
#ifdef FRAME_DEBUG
	printf ("%d freed frame %p\n", thread_tid (), frame_to_phys (f));
#endif

	if (!frame_is_write (f))
		{
			/* Discard the content. Nothing more to do. */
			goto direct;
		}
	else
		{
			if (frame_is_filebacked (f))
				{
					/* Write to backing file. */
					struct map_file *mapfile = frame_mapfile (f);
					ASSERT (mapfile != NULL && mapfile->writable);
				
					frame_set_busy (f);
					lock_release (&frame_table_lock);
					/* Release frame_table_lock before I/O, for more concurrency. */
					
					lock_acquire (&filesys_lock);
					/* Write READ_BYTES at OFFSET to backing file. */
					size_t bytes_written = file_write_at (mapfile->backing_file, frame_to_phys (f), 
											 					 aux, frame_offset (f));
					ASSERT (aux == (int)bytes_written);
					lock_release (&filesys_lock);
				}
			else
				{
					/* Write to swap space. */
					frame_set_busy (f);
					lock_release (&frame_table_lock);
					
					swap_put_page (aux, frame_to_phys (f));
					swap_clear_busy (aux);
					/* Now access to this page can page-in from swap again. */
				}
		}
	
	/* Clean up the frame struct. */
	lock_acquire (&frame_table_lock);
direct:
	f->mapfile = 0;
	f->offset = 0;
	/* which also clears all flags. */

	/* Free the user page. */
	void *phys_addr = frame_to_phys (f);
	palloc_free_page (phys_addr);
	lock_release (&frame_table_lock);
}

/** Check FRAME's accessed bit, based on RM_LIST generated by
    frame_reverse_map(). If CLEAR is set, also clears the first access
		bit encountered. If HARD is set, do not early stop. */
bool
frame_accessed (struct frame *f, struct list *rm_list, bool clear, bool hard)
{
	bool accessed = false;

	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);

      /* Locate PTE. */
			void *upage = pg_round_down (vma->lower_bound + 
																	(frame_offset (f) - vma->offset));
			
			if (pagedir_is_accessed (vma->proc->pagedir, upage))
				{
					accessed = true;
					if (clear)
						pagedir_set_accessed (vma->proc->pagedir, upage, false);
					if (!hard)
						break;
				}
    }
	return accessed;
}

/** Check FRAME's dirty bit, based on RM_LIST generated by
    frame_reverse_map(). If CLEAR is set, also clears all dirty
		bits in PTEs. */
bool
frame_dirty (struct frame *f, struct list *rm_list, bool clear)
{
	bool dirty = false;

	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			void *upage = pg_round_down (vma->lower_bound + 
																	(frame_offset (f) - vma->offset));
			if (pagedir_is_dirty (vma->proc->pagedir, upage))
				{
					dirty = true;
					if (clear)
						pagedir_set_dirty (vma->proc->pagedir, upage, false);
				}
    }
	return dirty;
}

/** Get the READ_BYTES property of FRAME, if file-backed, by 
    checking RM_LIST. 
		A frame should be shared by VMAs that, given FRAME's offset 
		in backing file, agree on how many bytes from that offset 
		belong to this FRAME's content. 
		-1 is returned, if RM_LIST is empty, where READ_BYTES cannot
		be acquired. */
int
frame_get_read_bytes (struct frame *f, struct list *rm_list)
{
	int read_bytes = -1;

	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			int remaining = (int)(vma->filesize - frame_offset (f));
			ASSERT (remaining >= 0);
			int _read_bytes = remaining > PGSIZE ? PGSIZE : remaining;

			if (read_bytes == -1) 
				read_bytes = _read_bytes;
			else
				{
					ASSERT (read_bytes == _read_bytes);
				}
    }
	return read_bytes;
}

/** Invalidate all PTEs referencing FRAME, based on RM_LIST generated by
    frame_reverse_map(), filling the PFN field with AUX. */
void
frame_invalidate (struct frame *f, struct list *rm_list, int aux, int avl_flag)
{
	struct list_elem *e;
  for (e = list_begin (rm_list); e != list_end (rm_list);
       e = list_next (e))
    {
      struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			void *upage = pg_round_down (vma->lower_bound + 
																	(frame_offset (f) - vma->offset));
			
			pagedir_clear_page (vma->proc->pagedir, upage, aux, avl_flag);
    }
}


/** Build the reverse mapping from FRAME to all relevant VMAs (containing
    present PTEs referencing FRAME), recorded in RM_LIST.
		The mapfile_elem in VMA struct is repurposed to build RM_LIST, so for every
		VMA that is file-backed, the page_pool_lock of its MAP_FILE is grabbed
		to protect modifications on vma_list, and NOT RELEASED ON RETURN. This is 
		meant to then protect RM_LIST and the MAP_FILE (for example, no process
		should now create alias to this frame, which is otherwise not registered
		in RM_LIST, and is therefore not invalidated properly at eviction).

		When done with RM_LIST, frame_reverse_map_free() must be invoked
		to possibly release the page_pool_lock. */
void
frame_reverse_map (struct frame *f, struct list *rm_list)
{
	ASSERT (!frame_is_empty (f));
	void *phys_addr = frame_to_phys (f);

	if (frame_is_filebacked (f))
		{
			/* FRAME -> MAP_FILE -> VMA */
			struct map_file *mapfile = frame_mapfile (f);
			ASSERT (mapfile != NULL);

			/* Grab page_pool_lock to also protect vma_list. */
			lock_acquire (&mapfile->page_pool_lock);
			
			struct list_elem *e;
      for (e = list_begin (&mapfile->vma_list); e != list_end (&mapfile->vma_list);)
        {
          struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
          /* Does FRAME fall in the area described by VMA? */
					off_t offset = frame_offset (f);
					if (offset >= vma->offset && 
							offset < vma->offset + (vma->upper_bound - vma->lower_bound))
						{
							ASSERT (vma->proc->pagedir != NULL);
							/* We only want present PTEs that map this frame. */
							if (pagedir_get_page (vma->proc->pagedir, vma->lower_bound + offset - vma->offset)
									!= phys_addr)
								goto next;

							/* Detach from vma_list, onto RM_LIST. */
							e = list_remove (e);
							list_push_back (rm_list, &vma->mapfile_elem);
							continue;
						}
					next: 
					e = list_next (e);
        }
			/* page_pool_lock is not released! */
		}
	else if (frame_is_anon (f))
		{
			/* FRAME -> VMA */
			struct vm_area *vma = frame_anon_vma (f);
			off_t offset = frame_offset (f);
			if (offset >= vma->offset && 
					offset < vma->offset + (vma->upper_bound - vma->lower_bound))
				{
					ASSERT (vma->proc->pagedir != NULL);
					if (pagedir_get_page (vma->proc->pagedir, vma->lower_bound + offset - vma->offset) 
							!= phys_addr)
						return;
					
					if (vma_is_filebacked (vma))
						{
							lock_acquire (&vma->mapfile->page_pool_lock);
							list_remove (&vma->mapfile_elem);
							list_push_back (rm_list, &vma->mapfile_elem);
							/* page_pool_lock is not released! */
						}
					else
						list_push_back (rm_list, &vma->mapfile_elem);
				}
		}
}


/** Free up the reverse map RM_LIST built on FRAME, that is, if FRAME
    is file-backed, re-insert RM_LIST into MAP_FILE's vma_list, and 
		release page_pool_lock. */
void
frame_reverse_map_free (struct frame *f, struct list *rm_list)
{
	ASSERT (!frame_is_empty (f));
	if (frame_is_filebacked (f))
		{
			struct map_file *mapfile = frame_mapfile (f);
			ASSERT (mapfile != NULL);
			ASSERT (lock_held_by_current_thread (&mapfile->page_pool_lock));

			/* Gracefully re-insert the entire list. */
			list_splice (list_end (&mapfile->vma_list),
									 list_begin (rm_list), list_end (rm_list));
			lock_release (&mapfile->page_pool_lock);
		}
	if (frame_is_anon (f))
		{
			ASSERT (list_size (rm_list) <= 1);
			struct list_elem *e = list_empty (rm_list) ? NULL : list_begin (rm_list);
			if (e == NULL)
				return;
			struct vm_area *vma = list_entry (e, struct vm_area, mapfile_elem);
			if (vma_is_filebacked (vma))
				{
					struct map_file *mapfile = vma->mapfile;
					ASSERT (lock_held_by_current_thread (&mapfile->page_pool_lock));
					list_remove (&vma->mapfile_elem);
					list_push_back (&mapfile->vma_list, &vma->mapfile_elem);
					lock_release (&mapfile->page_pool_lock);
				}
			else
				list_remove (&vma->mapfile_elem);
		}
}


/** Evict pages from the frame_table, hoping that at least CNT frames
    are free. Returns the number of pages actually evicted.
    
		The eviction policy is "clock algorithm" that approximates LFU (not LRU,
		in that pages with multiple accessed PTEs tend to stay in memory, i.e., 
		only one accessed bit is cleared at a time), with some heuristics.

		 - evict_clock_hand is maintained as the start of next eviction probing.
		 - For each frame encountered, clears the first accessed bit in its 
		   reverse mapping. If no accessed bit is found, evict.
		 - PINNED or BUSY frames are skipped.
		 - Free frames, if encountered, simply decreases the CNT requirement by 1.
		 - Early stops after scanning the entire frame_table once.

		 - With HARD, accessed bits are cleared for entire reverse mapping,
		   and the policy approximates LRU.

		For now, eviction only happens when free frame is needed but not found;
		first try evicting EVICT_MIN_CNT pages without HARD, and repeat with HARD
		if no single page is evicted. To prevent this routine from exclusively accessing
		frame_table for too long, frame_table_lock is released every EVICT_GRANULARITY
		frames scanned, allowing for more parallelism. 
		Note that with CNT and HARD option, it is straightforward to introduce 
		a daemon thread that is waken up when free memory is near full, which then
		runs in background to evict some pages (persumably without HARD), avoiding
		thrashing when completely OOM. */
int
frame_evict (size_t cnt, bool hard)
{
	ASSERT (!lock_held_by_current_thread (&frame_table_lock));

	/* Start of next eviction probing. Protected by frame_table_lock. */
	static size_t evict_clock_hand = 0;

	size_t evicted = 0, scanned = 0;
	struct list rm_list;
	list_init (&rm_list);
	while (evicted < cnt && scanned < frame_table_size)
		{
			/* Fine-grained locking. */
			if (!lock_held_by_current_thread (&frame_table_lock))
				lock_acquire (&frame_table_lock);
			scanned++;

			struct frame *f = &frame_table[evict_clock_hand];

			/* Accept already empty frame. */
			if (frame_is_empty (f))
				{
					evicted++;
					goto next;
				}

			/* Skip pinned or busy frame. */
			if (frame_pinned (f) || frame_is_busy (f))
				goto next;

			/* Acquire the reverse map. */
			frame_reverse_map (f, &rm_list);

			/* This re-check is necessary, because mapfile_get_page() might pin
			   an existing frame, after the previous PINNED check, but before
				 frame_reverse_map() holds the page_pool_lock. */
			if (frame_pinned (f))
				{
					frame_reverse_map_free (f, &rm_list);
					goto next;
				}

			/* Might see orphaned frame that has no present PTE reference, when
				(1) A shared frame that has COW triggered for every PTE reference;
						it remains in MAP_FILE's page_pool, but all PTEs record the
						new anonymous frames, not this original frame. 
				(2) frame_evict() interleaves with mapfile_remove_vma() (which decides
						to free frames). All those frames appear file-backed, but no PTE
						refer them. 
				For (1), due to the nature of COW, no write-back is needed, and we
				continue with freeing. For (2), we must leave it to mapfile_remove_vma()
				to properly write-back, because the READ_BYTES property is no longer
				available here. */
			if (list_empty (&rm_list))
				{
					ASSERT (frame_is_filebacked (f));
					if (list_empty (&frame_mapfile (f)->vma_list))
						{
							frame_reverse_map_free (f, &rm_list);
							goto next;
						}
					else
						{
							ASSERT (!frame_is_write (f));
						}
				}

			/* Decide whether to evict. */
			bool accessed = frame_accessed (f, &rm_list, true, hard);
			if (accessed)
				{
					frame_reverse_map_free (f, &rm_list);
					goto next;
				}
			
			/* Properly set up PTEs. */
			int aux = 0;
			if (frame_is_filebacked (f))
				{
					int read_bytes = frame_get_read_bytes (f, &rm_list);
					frame_invalidate (f, &rm_list, read_bytes, AVL_INFILE);

					/* Call frame_dirty() to clear DIRTY bits anyway. */
					if (frame_dirty (f, &rm_list, true))
						frame_set_write (f);

					struct map_file *mapfile = frame_mapfile (f);
					hash_delete (&mapfile->page_pool, &f->elem);
					aux = read_bytes;
				}
			else if (frame_is_anon (f))
				{
					int slot_idx = swap_reserve_slot ();
					if (slot_idx == SWAP_SLOT_INVALID)
						{
							/* In real OSes, OOM killing and eviction daemons are used to avoid
								 this case, where both full swap and memory is full. */
							PANIC ("frame_evict: swap is full");
						}
					swap_set_busy (slot_idx);
					frame_invalidate (f, &rm_list, slot_idx, AVL_INSWAP);
					/* By now, access to this frame should PF, and busy waits fetching
					   from swap, until we finish swapping out. */
					aux = slot_idx;
				}

			evicted++;
			evict_clock_hand = (evict_clock_hand + 1) % frame_table_size;
#ifdef FRAME_DEBUG
			printf ("%d Evicting frame %p, to slot %d\n", thread_tid (), frame_to_phys (f), aux);
#endif

			frame_reverse_map_free (f, &rm_list);
			/* And now, one of accesses to FRAME decides that FRAME is not 
				 in memory, allocates a new frame, then tries to read from file or
				 swap.
				 NOTE: if frame_free() decides to write back to file, we currently 
				 don't have a way to ensure this read happens after that write. 
				 This can cause MMAPed area to be inconsistent when shared, but is
				 tolerated in Lab3. We believe this can be addressed when we tackle
				 the file system.
				 For reading/writing the swap space, however, a busy wait mechanism
				 (like BUSY bit in a frame) is built in swap.c to solve the problem. */

			/* Finally, free the frame (it atomatically releases frame_table_lock). */
			frame_free (f, aux);
			continue;

			next:
			evict_clock_hand = (evict_clock_hand + 1) % frame_table_size;
			if (scanned % EVICT_GRANULARITY == 0)
				lock_release (&frame_table_lock);
		}
	if (lock_held_by_current_thread (&frame_table_lock))
		lock_release (&frame_table_lock);
	return evicted;
}

/** Hash funcs for FRAMEs in MAP_FILE's page_pool. */
unsigned 
frame_hash_func (const struct hash_elem *e, void *aux UNUSED)
{
	struct frame *f = hash_entry (e, struct frame, elem);
	return hash_int (((int) frame_offset (f)));
}

bool 
frame_less_func (const struct hash_elem *a,
                 const struct hash_elem *b,
                 void *aux UNUSED)
{
	struct frame *fa = hash_entry (a, struct frame, elem);
	struct frame *fb = hash_entry (b, struct frame, elem);

	return frame_offset (fa) < frame_offset (fb);
}
